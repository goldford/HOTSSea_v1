{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00e1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created by G Oldford \n",
    "# last modified March 25 2024\n",
    "\n",
    "# purpose: HOTSSea v1 plots model-obs profile prob dist fncs (pdfs) for temp and salin by domain\n",
    "#\n",
    "# data in: pickle files output from server pyap analysis - contains obs and mod for CTDs\n",
    "# data out: fig5 from manuscript, compares mean bias over each \n",
    "\n",
    "# notes: \n",
    "#  - input files are too large for guthub hosting so they are zipped\n",
    "#  - removed the pdfs for legibility\n",
    "#  - RUN216 old code for 1.02 and RUN213 old code for 1.01\n",
    "#  - code requires process to get stats from all ctd's using code on server\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283fd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import datetime as dt\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from matplotlib.path import Path as Path\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.path import Path as Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import cmocean.cm as cm\n",
    "\n",
    "import pkg_grid\n",
    "import pkg_geo\n",
    "\n",
    "\n",
    "go_changes = 'on'\n",
    "\n",
    "def get_profile_pdfs(scores, class4, bb, min_sample, reg, common_profiles, common_depths, std_thres=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : dict\n",
    "    class4 : dict\n",
    "    bb : dict\n",
    "        CTD region boundaries\n",
    "    min_sample : int\n",
    "        Minimum number of samples per layer. Exclude layers with fewer samples.\n",
    "    reg : str\n",
    "        CTD region name (from the CTD_analysis_domain...yaml)\n",
    "    std_thres : float, optional\n",
    "        Threshold for filtering out the outlier profiles. Outliers are detected as model-obs diff values\n",
    "        outside of `mean +- thres*stdev` range. No outlier detection by default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    bounds = Path(bb[reg])\n",
    "    # profiles within the region\n",
    "    # c4b = [c4 for k, c4 in class4.items() if bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    # TODO (2020-10-01): Check why Nones appear, fix, go back to the above\n",
    "    #kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "    #        if scores[k] is not None and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    if common_profiles is None:\n",
    "        kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "                if scores[k] is not None and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    else:\n",
    "        kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "                if str(k) in common_profiles and scores[k] is not None\n",
    "                and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    # early exit if no valid profiles\n",
    "    if len(kc4b) == 0:\n",
    "        return {'dep': [], 'bins': [], 'pdf': [], 'n': [], 'raw': [], 'outliers': []}, \\\n",
    "               {'dep': [], 'bins': [], 'pdf': [], 'n': [], 'raw': [], 'outliers': []}\n",
    "    kb = [_[0] for _ in kc4b]  # profile names\n",
    "    c4b = [_[1] for _ in kc4b]  # class4 values\n",
    "\n",
    "    if common_profiles is None:\n",
    "        z = [c4['dep'] for c4 in c4b]  # depth vectors for each profile\n",
    "    else:\n",
    "        z = [c4['dep'][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "    lens = [len(_) for _ in z]  # profile lengths\n",
    "    maxlen = max(lens)\n",
    "    z = z[np.argmax(lens)]\n",
    "    # tails of nans for each profile to make them same length\n",
    "    tails = [np.full(maxlen - len1, np.nan) for len1 in lens]\n",
    "\n",
    "    def var_prep(var):\n",
    "        \"\"\" Get diffs and determine outliers \"\"\"\n",
    "        if common_profiles is None:\n",
    "            v = [c4['model'][var] - c4['obs'][var] for c4 in c4b]  # collect model-obs diffs\n",
    "        else:\n",
    "            mvs = [c4['model'][var][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "            ovs = [c4['obs'][var][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "            v = [mv - ov for mv,ov in zip(mvs,ovs)]\n",
    "\n",
    "        v = [np.append(_, tail) for _, tail in zip(v, tails)]  # extend with nans to unify lengths\n",
    "        v = np.column_stack(v)  # array (ndep, nprofiles)\n",
    "        v_abs = np.abs(v)\n",
    "        binlim = np.ceil(np.nanmax(v_abs))  # bounds for the histogram\n",
    "        nv = np.sum(v_abs < binlim, axis=1)  # nans not counted here\n",
    "        # exclude undersampled layers\n",
    "        z_well = nv >= min_sample\n",
    "        v = v[z_well, :]\n",
    "        zv = z[z_well]\n",
    "        # TODO Return a message\n",
    "        # determine outliers\n",
    "        if (std_thres is None) | (std_thres == \"None\"):\n",
    "            iout = np.full(v.shape[1], False)\n",
    "            out_names = []\n",
    "        else:\n",
    "            v_std = np.nanstd(v, axis=1, keepdims=True)\n",
    "            v_mean = np.nanmean(v, axis=1, keepdims=True)\n",
    "            iout = np.abs(v - v_mean) > std_thres * v_std  # NOTE nans are not counted as outliers here\n",
    "            iout = np.any(iout, axis=0)  # profiles with outliers at ANY layer are marked as \"bad\"\n",
    "            out_names = [kb1 for kb1, i1 in zip(kb, iout) if i1]\n",
    "        return v, zv, ~iout, out_names\n",
    "\n",
    "    def var_pdf(v, zv):\n",
    "        \"\"\" PDF (normalized histogram) along dim=1 of a 2D array \"\"\"\n",
    "        if v.size == 0:  # shortcut if number of profiles is below min_sample\n",
    "            return [], [], [], [], []\n",
    "        binlim = np.ceil(np.nanmax(np.abs(v)))  # bounds for the histogram\n",
    "\n",
    "        # GO -20230607\n",
    "        num_bins = 100\n",
    "\n",
    "        if go_changes == 'on':\n",
    "          if num_bins > v.shape[1]:\n",
    "            num_bins = v.shape[1]\n",
    "          bins = np.linspace(-1.0 * binlim, binlim, num_bins)  # histogram bins\n",
    "        else:\n",
    "          bins = np.linspace(-1.0 * binlim, binlim, v.shape[1])  # histogram bins\n",
    "\n",
    "        pdf = [np.histogram(v1, bins=bins)[0] for v1 in v]  # histogram for each depth\n",
    "        pdf = np.vstack(pdf)  # (ndep, nbins)\n",
    "        nv = pdf.sum(axis=1)  # hist sum for each depth\n",
    "        pdf = pdf / nv[:, None]  # normalized histogram\n",
    "        # inz = nv != 0  # do not divide by zero\n",
    "        # pdf[inz, :] = pdf[inz, :] / nv[inz][:, None]  # normalized histogram\n",
    "\n",
    "        # exclude undersampled layers\n",
    "        z_good = nv >= min_sample\n",
    "        if not z_good.any():\n",
    "            zv = []\n",
    "            # v = np.zeros((0, 0)) + np.nan\n",
    "        else:\n",
    "            nv = nv[z_good]\n",
    "            zv = zv[z_good]\n",
    "            pdf = pdf[z_good, :]\n",
    "            v = v[z_good, :]\n",
    "            # v[v == 0.] = np.nan  # why this?\n",
    "        return zv, bins, pdf, nv, v\n",
    "\n",
    "    t, zt, igood_t, out_t = var_prep('T')\n",
    "    s, zs, igood_s, out_s = var_prep('S')\n",
    "    igood_ts = igood_t & igood_s\n",
    "    zt, bins_t, pdf_t, nt, t = var_pdf(t[:, igood_ts], zt)\n",
    "    zs, bins_s, pdf_s, ns, s = var_pdf(s[:, igood_ts], zs)\n",
    "\n",
    "    return {'dep': zt, 'bins': bins_t, 'pdf': pdf_t, 'n': nt, 'raw': t, 'outliers': out_t}, \\\n",
    "           {'dep': zs, 'bins': bins_s, 'pdf': pdf_s, 'n': ns, 'raw': s, 'outliers': out_s}\n",
    "\n",
    "# added by GO 20230607, started w/ copy of get_profile_pdfs\n",
    "# notes - not handling outliers, sample size filtering rn (ignore std_thres, min_sample)\n",
    "# this is now implemented in code edits on server\n",
    "def get_profile_means(scores, class4, bb, min_sample, reg, common_profiles, common_depths, std_thres=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    scores : dict\n",
    "    class4 : dict\n",
    "    bb : dict\n",
    "        CTD region boundaries\n",
    "    min_sample : int\n",
    "        Minimum number of samples per layer. Exclude layers with fewer samples.\n",
    "    reg : str\n",
    "        CTD region name (from the CTD_analysis_domain...yaml)\n",
    "    std_thres : float, optional\n",
    "        Threshold for filtering out the outlier profiles. Outliers are detected as model-obs diff values\n",
    "        outside of `mean +- thres*stdev` range. No outlier detection by default.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    {'mean_obs': [], 'mean_mod': [], 'std_obs': [], 'std_mod': []}\n",
    "\n",
    "    \"\"\"\n",
    "    bounds = Path(bb[reg])\n",
    "    # profiles within the region\n",
    "    # c4b = [c4 for k, c4 in class4.items() if bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    # TODO (2020-10-01): Check why Nones appear, fix, go back to the above\n",
    "    #kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "    #        if scores[k] is not None and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    if common_profiles is None:\n",
    "        kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "                if scores[k] is not None and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "    else:\n",
    "        kc4b = [(k, c4) for k, c4 in class4.items()\n",
    "                if str(k) in common_profiles and scores[k] is not None\n",
    "                and bounds.contains_point(scores[k]['ObsLocation'])]\n",
    "\n",
    "#     print(\"n profiles: \", len(kc4b))\n",
    "    # early exit if no valid profiles\n",
    "    if len(kc4b) == 0:\n",
    "        #return {'dep': [], 'bins': [], 'pdf': [], 'n': [], 'raw': [], 'outliers': []}, \\\n",
    "        #       {'dep': [], 'bins': [], 'pdf': [], 'n': [], 'raw': [], 'outliers': []}\n",
    "        return {'mean_obs': [], 'mean_mod': [], 'std_obs': [], 'std_mod': []}, \\\n",
    "               {'mean_obs': [], 'mean_mod': [], 'std_obs': [], 'std_mod': []}\n",
    "    kb = [_[0] for _ in kc4b]  # profile names\n",
    "    c4b = [_[1] for _ in kc4b]  # class4 values\n",
    "\n",
    "    if common_profiles is None:\n",
    "        z = [c4['dep'] for c4 in c4b]  # depth vectors for each profile\n",
    "    else:\n",
    "        z = [c4['dep'][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "    lens = [len(_) for _ in z]  # profile lengths\n",
    "    maxlen = max(lens)\n",
    "    z = z[np.argmax(lens)]\n",
    "    # tails of nans for each profile to make them same length\n",
    "    tails = [np.full(maxlen - len1, np.nan) for len1 in lens]\n",
    "\n",
    "    def var_prep(var): \n",
    "\n",
    "        \"\"\" Get diffs and determine outliers \"\"\"\n",
    "        if common_profiles is None:\n",
    "\n",
    "            # added by GO\n",
    "            mvs = [c4['model'][var] for c4 in c4b]\n",
    "            ovs = [c4['obs'][var] for c4 in c4b]\n",
    "\n",
    "            #v = [c4['model'][var] - c4['obs'][var] for c4 in c4b]  # collect model-obs diffs\n",
    "        else:\n",
    "            mvs = [c4['model'][var][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "            ovs = [c4['obs'][var][c4['dep'] <= common_depths[common_profiles == profile_name][0]] for profile_name,c4 in zip(kb,c4b)]\n",
    "\n",
    "            #v = [mv - ov for mv,ov in zip(mvs,ovs)]\n",
    "\n",
    "        #v = [np.append(_, tail) for _, tail in zip(v, tails)]  # extend with nans to unify lengths\n",
    "        #v = np.column_stack(v)  # array (ndep, nprofiles)\n",
    "        #v_abs = np.abs(v)\n",
    "\n",
    "        # added by GO\n",
    "        mvs = [np.append(_, tail) for _, tail in zip(mvs, tails)]  # extend with nans to unify lengths\n",
    "        mvs = np.column_stack(mvs)  # array (ndep, nprofiles)\n",
    "        ovs = [np.append(_, tail) for _, tail in zip(ovs, tails)]  # extend with nans to unify lengths\n",
    "        ovs = np.column_stack(ovs)  # array (ndep, nprofiles)\n",
    "\n",
    "\n",
    "        # not sure how to translate over yet - GO\n",
    "        # (filters if too few samples found at each depth)      \n",
    "        #binlim = np.ceil(np.nanmax(v_abs))  # bounds for the histogram\n",
    "        #nv = np.sum(v_abs < binlim, axis=1)  # nans not counted here\n",
    "\n",
    "        # exclude undersampled layers\n",
    "        #z_well = nv >= min_sample\n",
    "        #v = v[z_well, :]\n",
    "        #zv = z[z_well]\n",
    "\n",
    "        # not handling outliers here - GO\n",
    "        # TODO Return a message\n",
    "        # determine outliers\n",
    "        #if (std_thres is None) | (std_thres == \"None\"):\n",
    "        #    iout = np.full(v.shape[1], False)\n",
    "        #    out_names = []\n",
    "        #else:\n",
    "\n",
    "        #    v_std = np.nanstd(v, axis=1, keepdims=True)\n",
    "        #    v_mean = np.nanmean(v, axis=1, keepdims=True)\n",
    "        #    \n",
    "        #    iout = np.abs(v - v_mean) > std_thres * v_std  # NOTE nans are not counted as outliers here\n",
    "        #    iout = np.any(iout, axis=0)  # profiles with outliers at ANY layer are marked as \"bad\"\n",
    "        #    out_names = [kb1 for kb1, i1 in zip(kb, iout) if i1]\n",
    "\n",
    "        mvs_mean = np.nanmean(mvs, axis=1, keepdims=True)\n",
    "        mvs_std = np.nanstd(mvs, axis=1, keepdims=True)\n",
    "\n",
    "        ovs_mean = np.nanmean(ovs, axis=1, keepdims=True)\n",
    "        ovs_std = np.nanstd(ovs, axis=1, keepdims=True)\n",
    "\n",
    "        #return v, zv, ~iout, out_names\n",
    "        return ovs_mean, mvs_mean, ovs_std, mvs_std        \n",
    "\n",
    "\n",
    "    def var_pdf(v, zv):\n",
    "        \"\"\" PDF (normalized histogram) along dim=1 of a 2D array \"\"\"\n",
    "        if v.size == 0:  # shortcut if number of profiles is below min_sample\n",
    "            return [], [], [], [], []\n",
    "        binlim = np.ceil(np.nanmax(np.abs(v)))  # bounds for the histogram\n",
    "\n",
    "        # GO -20230607\n",
    "        num_bins = 100\n",
    "\n",
    "        if go_changes == 'on':\n",
    "          if num_bins > v.shape[1]:\n",
    "            num_bins = v.shape[1]\n",
    "          bins = np.linspace(-1.0 * binlim, binlim, num_bins)  # histogram bins\n",
    "        else:\n",
    "          bins = np.linspace(-1.0 * binlim, binlim, v.shape[1])  # histogram bins\n",
    "\n",
    "        pdf = [np.histogram(v1, bins=bins)[0] for v1 in v]  # histogram for each depth\n",
    "        pdf = np.vstack(pdf)  # (ndep, nbins)\n",
    "        nv = pdf.sum(axis=1)  # hist sum for each depth\n",
    "        pdf = pdf / nv[:, None]  # normalized histogram\n",
    "        # inz = nv != 0  # do not divide by zero\n",
    "        # pdf[inz, :] = pdf[inz, :] / nv[inz][:, None]  # normalized histogram\n",
    "\n",
    "        # exclude undersampled layers\n",
    "        z_good = nv >= min_sample\n",
    "        if not z_good.any():\n",
    "            zv = []\n",
    "            # v = np.zeros((0, 0)) + np.nan\n",
    "        else:\n",
    "            nv = nv[z_good]\n",
    "            zv = zv[z_good]\n",
    "            pdf = pdf[z_good, :]\n",
    "            v = v[z_good, :]\n",
    "            # v[v == 0.] = np.nan  # why this?\n",
    "        return zv, bins, pdf, nv, v\n",
    "\n",
    "\n",
    "    #t, zt, igood_t, out_t = var_prep('T') \n",
    "    #s, zs, igood_s, out_s = var_prep('S')\n",
    "\n",
    "    t_obs_mean, t_mod_mean, t_obs_std, t_mod_std = var_prep('T') \n",
    "    s_obs_mean, s_mod_mean, s_obs_std, s_mod_std = var_prep('S') \n",
    "\n",
    "    #igood_ts = igood_t & igood_s\n",
    "    #zt, bins_t, pdf_t, nt, t = var_pdf(t[:, igood_ts], zt)\n",
    "    #zs, bins_s, pdf_s, ns, s = var_pdf(s[:, igood_ts], zs)\n",
    "\n",
    "    return {'mean_obs': t_obs_mean, 'mean_mod': t_mod_mean, 'std_obs': t_obs_std, 'std_mod': t_mod_std}, \\\n",
    "           {'mean_obs': s_obs_mean, 'mean_mod': s_mod_mean, 'std_obs': s_obs_std, 'std_mod': s_mod_std}\n",
    "    \n",
    "def makeProfilePDFs(scores, class4, bb, minSample, reg):\n",
    "    # find max diff for T and S and depth for all profiles\n",
    "    maxDiffT = 0.\n",
    "    maxDiffS = 0.\n",
    "    maxDep = np.array([])\n",
    "    count = 0\n",
    "    for k in class4.keys():\n",
    "        # TODO (2020-10-01): Check why Nones appear, fix, remove the if-check\n",
    "        if scores[k] is None:\n",
    "            continue\n",
    "        # pick only belonging to this region\n",
    "        if not Path(bb[reg]).contains_point(scores[k]['ObsLocation']):\n",
    "            continue\n",
    "        diffT = np.abs(class4[k]['model']['T'] - class4[k]['obs']['T'])\n",
    "        if np.nanmax(diffT) > maxDiffT:\n",
    "            maxDiffT = np.nanmax(diffT)\n",
    "\n",
    "        diffS = np.abs(class4[k]['model']['S'] - class4[k]['obs']['S'])\n",
    "        if np.nanmax(diffS) > maxDiffS:\n",
    "            maxDiffS = np.nanmax(diffS)\n",
    "\n",
    "        if len(class4[k]['dep']) > len(maxDep):\n",
    "            maxDep = class4[k]['dep']\n",
    "        count += 1\n",
    "\n",
    "    diffGridT = np.linspace(-1. * np.ceil(maxDiffT), np.ceil(maxDiffT), count)\n",
    "    xT, yT = np.meshgrid(diffGridT, maxDep)\n",
    "    diffGridS = np.linspace(-1. * np.ceil(maxDiffS), np.ceil(maxDiffS), count)\n",
    "    xS, yS = np.meshgrid(diffGridS, maxDep)\n",
    "\n",
    "    # calculate PDFs (normalized histograms) for T and S diffs\n",
    "    pdf_T = np.zeros_like(xT)\n",
    "    pdf_S = np.zeros_like(xS)\n",
    "    count = 0\n",
    "    for k in class4.keys():  # go over profiles\n",
    "        # TODO (2020-10-01): Check why Nones appear, fix, remove the if-check\n",
    "        if scores[k] is None:\n",
    "            continue\n",
    "        if not Path(bb[reg]).contains_point(scores[k]['ObsLocation']):\n",
    "            continue\n",
    "\n",
    "        diffT = class4[k]['model']['T'] - class4[k]['obs']['T']\n",
    "        T1 = np.zeros_like(maxDep)\n",
    "        T1[:len(diffT)] = diffT\n",
    "        diffS = class4[k]['model']['S'] - class4[k]['obs']['S']\n",
    "        S1 = np.zeros_like(maxDep)\n",
    "        S1[:len(diffS)] = diffS\n",
    "        if count == 0:\n",
    "            T = T1\n",
    "            S = S1\n",
    "        else:\n",
    "            T = np.column_stack([T,T1])\n",
    "            S = np.column_stack([S,S1])\n",
    "\n",
    "        for i in range(len(diffT)):  # for each depth level\n",
    "            # TODO Bug: This counts nans and places them in the last bin\n",
    "            indT = np.digitize(diffT[i], diffGridT) - 1\n",
    "            # if i == len(maxDep)-1 and indT == len(diffGridT)-1:\n",
    "            #     print(f\"=====count: {count}    diffT[i]: {diffT[i]}\")\n",
    "            indS = np.digitize(diffS[i], diffGridS) - 1\n",
    "            pdf_T[i, indT] += 1\n",
    "            pdf_S[i, indS] += 1\n",
    "        count += 1\n",
    "\n",
    "    N = np.sum(pdf_T, axis=1)\n",
    "    for i in range(pdf_T.shape[0]):\n",
    "        pdf_T[i,:] = pdf_T[i,:]/np.sum(pdf_T[i,:])\n",
    "        pdf_S[i,:] = pdf_S[i,:]/np.sum(pdf_S[i,:])\n",
    "\n",
    "    pdf_T = pdf_T[N >= minSample,:]\n",
    "    pdf_S = pdf_S[N >= minSample,:]\n",
    "    maxDep = maxDep[N >= minSample]\n",
    "    xT,yT = np.meshgrid(diffGridT,maxDep)\n",
    "    xS,yS = np.meshgrid(diffGridS,maxDep)\n",
    "    if not (N >= minSample).any():\n",
    "        T = np.zeros((0,0)) + np.nan\n",
    "        S = np.zeros((0,0)) + np.nan\n",
    "    else:\n",
    "        T = T[N >= minSample,:]\n",
    "        S = S[N >= minSample,:]\n",
    "        N = N[N >= minSample]\n",
    "\n",
    "        T[T == 0.] = np.nan\n",
    "        S[S == 0.] = np.nan\n",
    "\n",
    "    return {'x': xT, 'y': yT, 'pdf': pdf_T, 'raw': T}, {'x': xS, 'y': yS, 'pdf': pdf_S, 'raw': S}, N, maxDep\n",
    "\n",
    "\n",
    "def get_bathy(opt):\n",
    "    lon, lat = pkg_grid.tgrid(opt['file_coord'])\n",
    "    lonf, latf = pkg_grid.fgrid(opt['file_coord'])\n",
    "    lonfe, latfe = pkg_geo.expandf(lonf,latf) # corner grid\n",
    "    depth = pkg_grid.bathymetry(opt['file_bathy'], maskfile=opt['file_mesh'])\n",
    "    bathy = dict(lon=lon, lat=lat, depth=depth, lonfe=lonfe, latfe=latfe)\n",
    "    return bathy\n",
    "\n",
    "def read_CTD_domain(domain_file):\n",
    "    \n",
    "    data = load_yaml(domain_file)\n",
    "    \n",
    "#     try:\n",
    "#         data = pkg_utils.load_yaml(domain_file)\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"WARNING:\\n CTD domain_file {} not found as a relative or absolute path\".format(domain_file))\n",
    "#         print(\" We will try to find the domain file directly from config; this fallback will be removed in future cleanups\")\n",
    "#         data = pkg_utils.load_config_yaml(domain_file)\n",
    "\n",
    "    dep = np.asarray(data['depth']).astype(float)\n",
    "    depth_levels = [[dep[i],dep[i+1]] for i in range(len(dep)-1)]\n",
    "\n",
    "    coords = {}\n",
    "    for c in data['polygon_coords'].keys():\n",
    "        coords[c] = np.asarray(data['polygon_coords'][c])\n",
    "        # make sure the polygon is closed\n",
    "        if not np.all(coords[c][-1] == coords[c][0]):\n",
    "            coords[c] = np.vstack([coords[c], coords[c][0, :]])\n",
    "\n",
    "    return depth_levels,coords\n",
    "\n",
    "\n",
    "# def load_config_yaml(f):\n",
    "#     configpath = os.path.normpath(os.path.dirname(__file__) + \"/../config\")\n",
    "#     yamldata = load_yaml(os.path.join(configpath, f))\n",
    "#     return yamldata\n",
    "\n",
    "\n",
    "def load_yaml(yamlfile):\n",
    "    \"\"\" Helper to load a YAML\n",
    "    \"\"\"\n",
    "    def date_to_datetime(loader, node):\n",
    "        \"\"\" The default YAML loader interprets YYYY-MM-DD as a datetime.date object\n",
    "            Here we override this with a datetime.datetime object with implicit h,m,s=0,0,0 \"\"\"\n",
    "        d = yaml.constructor.SafeConstructor.construct_yaml_timestamp(loader,node)\n",
    "        if type(d) is dt.date:\n",
    "            d = dt.datetime.combine(d, dt.time(0, 0, 0))\n",
    "        return d\n",
    "    yaml.constructor.SafeConstructor.yaml_constructors[u'tag:yaml.org,2002:timestamp'] = date_to_datetime\n",
    "    with open(yamlfile, 'r') as ya:\n",
    "        try:\n",
    "            yamldata = yaml.safe_load(ya)\n",
    "        except Exception as e:\n",
    "            print(\"Error importing YAML file {} {})\".format(yamlfile,e))\n",
    "            raise\n",
    "    return yamldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce7620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading domain file and computing bounding boxes...\n",
      "Plotting DI...\n"
     ]
    }
   ],
   "source": [
    "domain_f = '..//..//data//eval//eval_config//CTD_analysis_domain_config_template-SalishSea.yml' \n",
    "coords_f = '..//..//data//grid//coordinates_salishsea_1500m.nc' \n",
    "\n",
    "bathy_f = '..//..//data//bathymetry//bathy_salishsea_1500m_20210706.nc'\n",
    "mesh_f = '..//..//data//mesh mask//mesh_mask_20210406.nc' \n",
    "\n",
    "skip_PS = True\n",
    "\n",
    "# SalishSea1500-RUN216.yaml\n",
    "\n",
    "min_sample = 5 # per region (default)\n",
    "std_thres = 2 \n",
    "formats = 'PNG'\n",
    "\n",
    "# Get bounding boxes and map zooms\n",
    "print('Loading domain file and computing bounding boxes...')\n",
    "# depth_levels, coords = pkg_analyze_cast.read_CTD_domain(opt['analysis']['CTD']['domain_file'])\n",
    "depth_levels, coords = read_CTD_domain(domain_f)\n",
    "bb = {}\n",
    "for k in coords.keys():\n",
    "    # make sure the polygon is closed\n",
    "    if np.all(coords[k][-1] == coords[k][0]):\n",
    "        bb[k] = coords[k]\n",
    "    else:\n",
    "        bb[k] = np.vstack([coords[k], coords[k][0, :]])\n",
    "\n",
    "# bathy = pkg_plot.get_bathy(opt)\n",
    "lon, lat = pkg_grid.tgrid(coords_f)\n",
    "lonf, latf = pkg_grid.fgrid(coords_f)\n",
    "lonfe, latfe = pkg_geo.expandf(lonf,latf) # corner grid\n",
    "depth = pkg_grid.bathymetry(bathy_f, maskfile=mesh_f)\n",
    "bathy = dict(lon=lon, lat=lat, depth=depth, lonfe=lonfe, latfe=latfe)\n",
    "\n",
    "\n",
    "modelruns_info = {'SalishSea1500-RUN203': {'path': 'D:/temp_nemo/RUN203_PLOTS_SCORES/',\n",
    "                                            'colour': 'r', \n",
    "                                            'shortcode': 'RUN203', 'experiment': False}, \n",
    "                      'SalishSea1500-RUN216': {'path': 'D:/temp_nemo/RUN216/',\n",
    "                                            'colour': 'b', \n",
    "                                           'shortcode': 'RUN216', 'experiment': False}#,\n",
    "#                      'SalishSea500-201905': {'path': 'D:/temp_nemo/SS500/',\n",
    "#                                            'colour': 'g', \n",
    "#                                            'shortcode': '201905', 'experiment': True}\n",
    "                 }\n",
    "\n",
    "modelruns_info = {'SalishSea1500-RUN203': {'path': 'D:/nemo_outputs/ANALYSIS/SalishSea1500-RUN203/PROCESS/CTD/',\n",
    "                                            'colour': 'r', \n",
    "                                            'shortcode': 'RUN203', 'experiment': False}, \n",
    "                      'SalishSea1500-RUN216': {'path': 'D:/nemo_outputs/ANALYSIS/SalishSea1500-RUN216/PROCESS/CTD/',\n",
    "                                            'colour': 'b', \n",
    "                                           'shortcode': 'RUN216', 'experiment': False}#,\n",
    "#                      'SalishSea500-201905': {'path': 'D:/temp_nemo/SS500/',\n",
    "#                                            'colour': 'g', \n",
    "#                                            'shortcode': '201905', 'experiment': True}\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# manual setting of x-axis limits\n",
    "xLimT = 3.\n",
    "xLimS = 5.       #Go 20230411 commented line to use these below back in\n",
    "lw1 = 1        #GO 20230411 CRMSE\n",
    "lw2 = 1      #GO 20230411  +/- lines\n",
    "fs1 = 7\n",
    "fs2 = 6 #  fontsize tics\n",
    "gamma1 = 0.65      #Go 20230411 for powernorm color norm\n",
    "fig_dim_1 = 8 #w\n",
    "fig_dim_2 = 11\n",
    "# fig_dim_2 = 3 #h\n",
    "min_x_salt_mean = 22 # for plotting, found through trial and error across all plots\n",
    "max_x_salt_mean = 34\n",
    "min_x_temp_mean = 6\n",
    "max_x_temp_mean = 13\n",
    "grd_hspace = 2 # space of axis vert reserved for labels\n",
    "grd_wspace = 0.3\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(fig_dim_1, fig_dim_2))\n",
    "\n",
    "#gs = gridspec.GridSpec(1, 6)\n",
    "len_bb = len(bb.keys())\n",
    "if skip_PS == True: \n",
    "    len_bb -= 1\n",
    "gs = gridspec.GridSpec(5*len_bb+2, 10,hspace=grd_hspace,wspace=grd_wspace)\n",
    "\n",
    "\n",
    "\n",
    "# for region in bounding box list\n",
    "n_reg = 0\n",
    "for reg in bb.keys():\n",
    "    if reg == \"PS\" and skip_PS == True:\n",
    "        print('Skipping PS')\n",
    "        n_reg+=1\n",
    "        continue\n",
    "    else:\n",
    "        print('Plotting ' + reg + '...')\n",
    "    \n",
    "#     if n_reg >= 2: #debug\n",
    "#         continue\n",
    "    \n",
    "    # set which gridspec row for subplots\n",
    "    gs_row_st = 0+n_reg*5\n",
    "    gs_row_en = 5+n_reg*5\n",
    "    \n",
    "    # GO 20230815- loop to show the bias correction\n",
    "    n_run = 0\n",
    "    for run in modelruns_info.keys():\n",
    "        \n",
    "        path1 = modelruns_info[run]['path']\n",
    "        CTD_f = 'CTDcast_metrics_hindcast_fewdepths.pickle'\n",
    "        with open(os.path.join(path1,CTD_f), 'rb') as fid:\n",
    "            CTD_scores = pickle.load(fid)\n",
    "\n",
    "        # returns pcolormeshes for plots\n",
    "        plotT,plotS,N,maxDep = makeProfilePDFs(CTD_scores[run]['scores'],CTD_scores[run]['class4'],bb,min_sample,reg)\n",
    "        plotT, plotS = get_profile_pdfs(CTD_scores[run]['scores'], CTD_scores[run]['class4'], bb, min_sample, reg, None, None, std_thres)\n",
    "\n",
    "        if len(plotT['dep']) == 0 and len(plotS['dep']) == 0:\n",
    "            print(f'Region {reg} has no profiles. Not making a plot for this')\n",
    "            continue\n",
    "\n",
    "        # GO added below\n",
    "        plotT_mean, plotS_mean = get_profile_means(CTD_scores[run]['scores'], CTD_scores[run]['class4'], bb, min_sample, reg, None, None, std_thres)\n",
    "        # for now take dep from fnc above\n",
    "        plotT_mean['dep'] = plotT['dep']\n",
    "        plotS_mean['dep'] = plotS['dep']\n",
    "\n",
    "        # temporarily turn this off - GO 20230607\n",
    "        #save_outliers(plotT['outliers'], plotS['outliers'], os.path.join(outdir, f\"{reg}_outliers.txt\")     \n",
    "\n",
    "        axt_mean = plt.subplot(gs[gs_row_st:gs_row_en, :2])\n",
    "        axs_mean = plt.subplot(gs[gs_row_st:gs_row_en, 2:4], sharey=axt_mean)\n",
    "\n",
    "        ax = [axt_mean, axs_mean]   \n",
    "\n",
    "        # plot the mean temps\n",
    "#         ax[0].set_ylabel('Pressure (dBar)', fontsize=fs1, labelpad=2)\n",
    "        ax[0].set_ylabel('Depth (m)', fontsize=fs1, labelpad=2)\n",
    "        dep_n = len(plotT_mean['dep'])\n",
    "        \n",
    "        if n_run == 0: \n",
    "            color_l = 'k'\n",
    "            line_l = \":\"\n",
    "        else: \n",
    "            color_l = 'r'\n",
    "            line_l = \"--\"\n",
    "        \n",
    "        obs_t_mean = plotT_mean['mean_obs'][0:dep_n]\n",
    "        ax[0].plot(obs_t_mean, plotT_mean['dep'], 'k-')\n",
    "        mod_t_mean = plotT_mean['mean_mod'][0:dep_n]\n",
    "        ax[0].plot(mod_t_mean, plotT_mean['dep'], color_l+line_l)\n",
    "        \n",
    "        mean_obs = Line2D([], [], color='k', lw=lw2)\n",
    "        mean_mod = Line2D([], [], color='k', ls='--', lw=lw2)\n",
    "#         ax[0].legend([mean_obs, mean_mod], ['Obs.', 'Mod.'], fontsize=fs1, bbox_to_anchor=[0.4, -0.4])\n",
    "#         #ax[0].legend([mean_obs, mean_mod], ['Obs.', 'Mod.'], fontsize=fs1, loc='upper left')\n",
    "\n",
    "\n",
    "        # plot the mean salin\n",
    "        dep_n = len(plotS_mean['dep'])\n",
    "        obs_s_mean = plotS_mean['mean_obs'][0:dep_n]\n",
    "        ax[1].plot(obs_s_mean, plotS_mean['dep'], 'k-')\n",
    "        mod_s_mean = plotS_mean['mean_mod'][0:dep_n]\n",
    "        ax[1].plot(mod_s_mean, plotS_mean['dep'], color_l+line_l)\n",
    "\n",
    "        # don't plot values deeper than 2000 m, arbitrarily based on depth of ARGO profiles\n",
    "        zMax = np.min(np.array([2000., max(plotT['dep'][-1], plotS['dep'][-1])]))\n",
    "\n",
    "        xlabs = [u'$\\Delta$ Temp.\\nMod. - Obs. ($^\\circ$C)',\n",
    "                 u'$\\Delta$ Salin.\\nMod. - Obs. (psu)']\n",
    "\n",
    "        bias = Line2D([], [], color='k', lw=lw2)\n",
    "        rms = Line2D([], [], color='k', ls='--', lw=lw2)\n",
    "#         ax[1].legend([bias, rms], ['Bias', 'CRMSE'], fontsize=fs1, bbox_to_anchor=[0.2, -0.4])\n",
    "#         #ax[1].legend([bias, rms], ['Bias', 'CRMSE'], fontsize=fs1, loc='upper left')\n",
    "\n",
    "        # # of obs plot\n",
    "        ax.append(plt.subplot(gs[gs_row_st:gs_row_en,4], sharey=axt_mean))\n",
    "        # not sure what this is for -GO\n",
    "        #ax.append(fig.add_axes([0.8, 0.3, 0.02, 0.4]))\n",
    "        ax[2].plot(plotT['n'], plotT['dep'], color='k', label='T')\n",
    "\n",
    "        if len(plotT['n']) != len(plotS['n']) or np.any(plotT['n'] != plotS['n']):\n",
    "            ax[2].plot(plotS['n'], plotS['dep'], '--', color='k', label='S')\n",
    "            #ax[4].legend(fontsize=fs1)\n",
    "        \n",
    "        ax[2].grid(which='both')\n",
    "        ax[2].set_ylim(0., zMax)\n",
    "        ax[2].set_xlim(0, 1.05 * max(plotT['n'].max(), plotS['n'].max()))\n",
    "\n",
    "        ax[0].invert_yaxis()\n",
    "\n",
    "#         if reg == \"DI\":\n",
    "#             subtitle = \"Discovery Is. (DI)\" \n",
    "#         elif reg == \"SGN\":\n",
    "#             subtitle = \"Strait of Georgia North (SGN)\"\n",
    "#         elif reg == \"SGS\":\n",
    "#             subtitle = \"Strait of Georgia South (SGS)\"\n",
    "#         elif reg == \"GI\":\n",
    "#             subtitle = \"Southern Gulf Islands (GI)\"\n",
    "#         elif reg == \"HS\":\n",
    "#             subtitle = \"Haro Strait (HS)\"\n",
    "#         elif reg == \"PS\":\n",
    "#             subtitle = \"Puget Sound (PS)\"\n",
    "#         elif reg == \"JFS\":\n",
    "#             subtitle = \"Juan de Fuca Strait (JFS)\"\n",
    "#         else:\n",
    "#             subtitle = reg\n",
    "        subtitle=reg\n",
    "\n",
    "        ax[0].set_title(subtitle,fontdict={'fontsize':fs1},loc='right',y=0.4,x=-0.4)\n",
    "\n",
    "        # format the axes for means\n",
    "        ax_n = 0\n",
    "        for a in [ax[0], ax[1]]:\n",
    "            if ax_n == 0:\n",
    "                a.set_xlim(min_x_temp_mean,max_x_temp_mean)\n",
    "                ticks = np.arange(min_x_temp_mean, max_x_temp_mean+1, 2)\n",
    "            else:\n",
    "                a.set_xlim(min_x_salt_mean,max_x_salt_mean)\n",
    "                ticks = np.arange(min_x_salt_mean, max_x_salt_mean+3, 3)\n",
    "            a.set_xticks(ticks)\n",
    "    #             a.grid(which='both')\n",
    "            plt.sca(a)\n",
    "            plt.tick_params(axis='x', labelsize=fs2, pad=1)\n",
    "            plt.tick_params(axis='y', labelsize=fs2)\n",
    "\n",
    "            ax_n+=1\n",
    "\n",
    "        for a in [ax[1], ax[2]]:\n",
    "            plt.sca(a)\n",
    "            plt.tick_params(axis='y', labelleft=False)\n",
    "            plt.tick_params(axis='x', labelsize=fs2, pad=1)\n",
    "\n",
    "        for a in [ax[0], ax[1], ax[2]]:\n",
    "            plt.sca(a)\n",
    "            plt.grid(which='both', linestyle=\":\")\n",
    "\n",
    "        \n",
    "        if n_reg == len_bb - 1:\n",
    "            ax[0].set_xlabel(\"Mean\\nTemp. ($^\\circ$C)\", fontsize=fs1)\n",
    "            ax[1].set_xlabel(\"Mean\\nSalin. (PSU)\", fontsize=fs1)\n",
    "            ax[2].set_xlabel('# of \\nobservations', fontsize=fs1)\n",
    "            \n",
    "\n",
    "            #title w/ region and model run\n",
    "#             ax[4].text(1.18, 0.98, reg + '\\n' + run, ha='left', va='top', transform=ax[2].transAxes)\n",
    "            # text above is in data units for position so use title\n",
    "        \n",
    "#         # Tag the plots if outliers were excluded.\n",
    "#         # With no detected outliers, the plots from the \"no-outlier\" run are overwritten\n",
    "#         if len(plotT['outliers']) > 0 or len(plotS['outliers']) > 0:\n",
    "#             tag = '_without_outliers'\n",
    "#         else:\n",
    "#             tag = ''\n",
    "#     save_path = os.path.join(outdir, reg + f'_ProfilePDFAndMean{tag}')\n",
    "#     pkg_plot.figure_saver(save_path, formats, fig, dpi=450, bbox_inches='tight')\n",
    "# plt.close()\n",
    "        n_run += 1\n",
    "    n_reg += 1\n",
    "    \n",
    "letters = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)', \n",
    "          '(i)', '(j)', '(k)', '(l)', '(m)', '(n)', '(o)', '(p)', '(q)', '(r)']\n",
    "let_clr = 'k'\n",
    "let_y = 0.87\n",
    "let_xs = [0.1, 0.1, 0.2,\n",
    "          0.1, 0.1, 0.2,\n",
    "          0.1, 0.1, 0.2,\n",
    "          0.1, 0.1, 0.2,\n",
    "          0.1, 0.1, 0.2,\n",
    "          0.1, 0.1, 0.2\n",
    "         ]\n",
    "l = 0\n",
    "for ax in fig.get_axes():\n",
    "    letter = letters[l]\n",
    "    let_x = let_xs[l]\n",
    "    ax.text(let_x, let_y, letter, transform=ax.transAxes, ha='center', color=let_clr, fontsize=9, zorder=1000)\n",
    "    l += 1\n",
    "\n",
    "plt.savefig('..//..//figs//Fig05.pdf', dpi=350)\n",
    "plt.savefig('..//..//figs//Fig05.png', dpi=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7f231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0d17f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff86c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
